Specified arguments: Namespace(small_subset=True, num_epochs=5, lr=5e-05, batch_size=8, prefix_length=128)
Loading the tokenizer...
 >>>>>>>> Initializing the data loaders ... 
Loading the model ...
Moving model to device ...cuda
 >>>>>>>>  Starting training ... 
 >>>>>>>>  Initializing optimizer
Epoch 1 training:
0it [00:00, ?it/s]STAGE:2024-04-14 18:10:35 3640614:3640614 ActivityProfilerController.cpp:314] Completed Stage: Warm Up
past key values is None
STAGE:2024-04-14 18:10:35 3640614:3640614 ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-04-14 18:10:35 3640614:3640614 ActivityProfilerController.cpp:324] Completed Stage: Post Processing
0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cs601-zxia15/NLP_final_project/executables/../src/gpt2_finetune_but_classification_backbone.py", line 262, in <module>
    
  File "/home/cs601-zxia15/NLP_final_project/executables/../src/gpt2_finetune_but_classification_backbone.py", line 164, in train
    predictions = mymodel(input_ids, attention_mask)
  File "/home/cs601-zxia15/.conda/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cs601-zxia15/.conda/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cs601-zxia15/NLP_final_project/executables/../src/gpt2_finetune_but_classification_backbone.py", line 20, in forward
    output = self.model(input_ids=input_ids, attention_mask=attention_mask)
  File "/home/cs601-zxia15/.conda/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cs601-zxia15/.conda/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cs601-zxia15/NLP_final_project/src/GPT2ForwardBackward/modeling_opengpt2.py", line 1014, in forward
    transformer_outputs = self.transformer(
  File "/home/cs601-zxia15/.conda/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cs601-zxia15/.conda/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cs601-zxia15/NLP_final_project/src/GPT2ForwardBackward/modeling_opengpt2.py", line 861, in forward
    outputs = block(
  File "/home/cs601-zxia15/.conda/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cs601-zxia15/.conda/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cs601-zxia15/NLP_final_project/src/GPT2ForwardBackward/modeling_opengpt2.py", line 374, in forward
    attn_outputs = self.attn(
  File "/home/cs601-zxia15/.conda/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cs601-zxia15/.conda/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cs601-zxia15/NLP_final_project/src/GPT2ForwardBackward/modeling_opengpt2.py", line 303, in forward
    attn_outputs = self._attn(query, key, value, attention_mask, head_mask, output_attentions)
  File "/home/cs601-zxia15/NLP_final_project/src/GPT2ForwardBackward/modeling_opengpt2.py", line 213, in _attn
    w = w / (float(v.size(-1)) ** 0.5)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 9.50 GiB of which 270.00 MiB is free. Process 3550610 has 6.72 GiB memory in use. Process 3614255 has 3.51 GiB memory in use. Including non-PyTorch memory, this process has 9.22 GiB memory in use. Of the allocated memory 8.64 GiB is allocated by PyTorch, and 479.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
